diff --git a/kernel/sched/fair.c b/kernel/sched/fair.c
index c2980e8..979d733 100644
--- a/kernel/sched/fair.c
+++ b/kernel/sched/fair.c
@@ -5942,6 +5942,8 @@ struct sg_lb_stats {
 	unsigned int sum_nr_running; /* Nr tasks running in the group */
 	unsigned int idle_cpus;
 	unsigned int group_weight;
+   unsigned long min_load;
+   unsigned long max_load;
 	enum group_type group_type;
 	int group_no_capacity;
 #ifdef CONFIG_NUMA_BALANCING
@@ -6261,6 +6263,8 @@ static inline void update_sg_lb_stats(struct lb_env *env,
 			bool *overload)
 {
 	unsigned long load;
+   unsigned long min_load = ULONG_MAX;
+   unsigned long max_load = 0UL;
 	int i;
 
 	memset(sgs, 0, sizeof(*sgs));
@@ -6274,6 +6278,11 @@ static inline void update_sg_lb_stats(struct lb_env *env,
 		else
 			load = source_load(i, load_idx);
 
+      if (load < min_load)
+         min_load = load;
+      if(load > max_load)
+         max_load = load;
+
 		sgs->group_load += load;
 		sgs->group_usage += get_cpu_usage(i);
 		sgs->sum_nr_running += rq->cfs.h_nr_running;
@@ -6294,6 +6303,9 @@ static inline void update_sg_lb_stats(struct lb_env *env,
 	sgs->group_capacity = group->sgc->capacity;
 	sgs->avg_load = (sgs->group_load*SCHED_CAPACITY_SCALE) / sgs->group_capacity;
 
+   sgs->min_load = min_load;
+   sgs->max_load = max_load;
+
 	if (sgs->sum_nr_running)
 		sgs->load_per_task = sgs->sum_weighted_load / sgs->sum_nr_running;
 
@@ -6323,14 +6335,15 @@ static bool update_sd_pick_busiest(struct lb_env *env,
 {
 	struct sg_lb_stats *busiest = &sds->busiest_stat;
 
-	if (sgs->group_type > busiest->group_type)
-		return true;
+   /* Not sure we want to keep that or not */
+	/*if (sgs->group_type > busiest->group_type)
+		return true;*/
 
-	if (sgs->group_type < busiest->group_type)
-		return false;
+   if (sgs->min_load <= busiest->min_load)
+      return true;
 
-	if (sgs->avg_load <= busiest->avg_load)
-		return false;
+   if (sgs->group_type == group_imbalanced)
+      return true;
 
 	/* This is the busiest node in its class. */
 	if (!(env->sd->flags & SD_ASYM_PACKING))
@@ -6589,63 +6602,14 @@ static inline void calculate_imbalance(struct lb_env *env, struct sd_lb_stats *s
 	local = &sds->local_stat;
 	busiest = &sds->busiest_stat;
 
-	if (busiest->group_type == group_imbalanced) {
-		/*
-		 * In the group_imb case we cannot rely on group-wide averages
-		 * to ensure cpu-load equilibrium, look at wider averages. XXX
-		 */
-		busiest->load_per_task =
-			min(busiest->load_per_task, sds->avg_load);
-	}
-
-	/*
-	 * In the presence of smp nice balancing, certain scenarios can have
-	 * max load less than avg load(as we skip the groups at or below
-	 * its cpu_capacity, while calculating max_load..)
-	 */
-	if (busiest->avg_load <= sds->avg_load ||
-	    local->avg_load >= sds->avg_load) {
-		env->imbalance = 0;
-		return fix_small_imbalance(env, sds);
-	}
-
-	/*
-	 * If there aren't any idle cpus, avoid creating some.
-	 */
-	if (busiest->group_type == group_overloaded &&
-	    local->group_type   == group_overloaded) {
-		load_above_capacity = busiest->sum_nr_running *
-					SCHED_LOAD_SCALE;
-		if (load_above_capacity > busiest->group_capacity)
-			load_above_capacity -= busiest->group_capacity;
-		else
-			load_above_capacity = ~0UL;
-	}
-
-	/*
-	 * We're trying to get all the cpus to the average_load, so we don't
-	 * want to push ourselves above the average load, nor do we wish to
-	 * reduce the max loaded cpu below the average load. At the same time,
-	 * we also don't want to reduce the group load below the group capacity
-	 * (so that we can implement power-savings policies etc). Thus we look
-	 * for the minimum possible imbalance.
-	 */
-	max_pull = min(busiest->avg_load - sds->avg_load, load_above_capacity);
+   if(local->min_load >= busiest->max_load) {
+      env->imbalance = 0;
+   }
+   else {
+      env->imbalance = (busiest->max_load - local->min_load) / 2;
+   }
 
-	/* How much load to actually move to equalise the imbalance */
-	env->imbalance = min(
-		max_pull * busiest->group_capacity,
-		(sds->avg_load - local->avg_load) * local->group_capacity
-	) / SCHED_CAPACITY_SCALE;
-
-	/*
-	 * if *imbalance is less than the average load per runnable task
-	 * there is no guarantee that any tasks will be moved so we'll have
-	 * a think about bumping its value to force at least one task to be
-	 * moved
-	 */
-	if (env->imbalance < busiest->load_per_task)
-		return fix_small_imbalance(env, sds);
+   return;
 }
 
 /******* find_busiest_group() helpers end here *********************/
@@ -6707,40 +6671,14 @@ static struct sched_group *find_busiest_group(struct lb_env *env)
 	    busiest->group_no_capacity)
 		goto force_balance;
 
-	/*
-	 * If the local group is busier than the selected busiest group
-	 * don't try and pull any tasks.
-	 */
-	if (local->avg_load >= busiest->avg_load)
-		goto out_balanced;
-
-	/*
-	 * Don't pull any tasks if this group is already above the domain
-	 * average load.
-	 */
-	if (local->avg_load >= sds.avg_load)
-		goto out_balanced;
-
-	if (env->idle == CPU_IDLE) {
-		/*
-		 * This cpu is idle. If the busiest group is not overloaded
-		 * and there is no imbalance between this and busiest group
-		 * wrt idle cpus, it is balanced. The imbalance becomes
-		 * significant if the diff is greater than 1 otherwise we
-		 * might end up to just move the imbalance on another group
-		 */
-		if ((busiest->group_type != group_overloaded) &&
-				(local->idle_cpus <= (busiest->idle_cpus + 1)))
-			goto out_balanced;
-	} else {
-		/*
-		 * In the CPU_NEWLY_IDLE, CPU_NOT_IDLE cases, use
-		 * imbalance_pct to be conservative.
-		 */
-		if (100 * busiest->avg_load <=
-				env->sd->imbalance_pct * local->avg_load)
-			goto out_balanced;
-	}
+   if(local->min_load < busiest->min_load)
+   {
+      goto force_balance;
+   }
+   else
+   {
+      goto out_balanced;
+   }
 
 force_balance:
 	/* Looks like there is an imbalance. Compute it */
